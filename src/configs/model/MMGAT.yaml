# Enhanced hyperparameter configuration for MMGAT

embedding_size: 64
feat_embed_dim: 64

# Increased number of layers for better representation
n_layers: 3

# Optimized dropout rates
dropout: [0.1, 0.2, 0.3]

# Refined regularization weights
reg_weight: [1e-5, 5e-5, 1e-4]

# Increased number of attention heads
n_heads: 8

# Temperature parameter for InfoNCE loss
temperature: 0.07

# Modal fusion parameters
modal_fusion_layers: 2
modal_dropout: 0.1

# Graph attention parameters
gat_dropout: 0.1
gat_heads: 8
gat_concat: True

# Training parameters
learning_rate: 0.001
weight_decay: 0.0001
batch_size: 1024

hyper_parameters: ["dropout", "reg_weight"]